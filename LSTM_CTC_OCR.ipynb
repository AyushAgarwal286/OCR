{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import h5py\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining different variables\n",
    "data_dir = '../data/NIST19_combined/'\n",
    "img_width = 140\n",
    "img_height = 28\n",
    "timesteps_size = 140\n",
    "num_hidden_units = 256\n",
    "num_classes = 63\n",
    "\n",
    "# reading the maps\n",
    "label_cls_name_map = {}\n",
    "label_name_cls_map = {}\n",
    "with open('ocr_checkpoints/label_cls_name.json', 'r') as f:\n",
    "    label_cls_name_map = json.loads(f.read())\n",
    "    \n",
    "for k,v in label_cls_name_map.iteritems():\n",
    "    label_name_cls_map[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully !!!\n"
     ]
    }
   ],
   "source": [
    "# train dataset inputs and targets\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "train_image_names = os.listdir(os.path.join(data_dir,'train_images'))\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_train_annotations.csv'))\n",
    "for train_image_name in train_image_names:\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'train_images'), train_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0) # reading the image as 1 channel\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height), interpolation = cv2.INTER_AREA).T\n",
    "    train_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(train_df[train_df['filename']==train_image_name]['class'].tolist())\n",
    "    train_targets.append(target_)\n",
    "    \n",
    "# test dataset inputs and targets\n",
    "test_inputs = []\n",
    "test_targets = []\n",
    "test_image_names = os.listdir(os.path.join(data_dir,'test_images'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_test_annotations.csv'))\n",
    "for test_image_name in test_image_names:\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'test_images'), test_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0)\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height), interpolation = cv2.INTER_AREA).T\n",
    "    test_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(test_df[test_df['filename']==test_image_name]['class'].tolist())\n",
    "    test_targets.append(target_)\n",
    "\n",
    "train_inputs_ = np.array(train_inputs)\n",
    "train_targets_ = np.array(train_targets)\n",
    "test_inputs_ = np.array(test_inputs)\n",
    "test_targets_ = np.array(test_targets)\n",
    "print 'Data loaded successfully !!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 140, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f044cb52410>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f044ca05390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 140, 28)\n",
      "(20000,)\n",
      "(2000, 140, 28)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print train_inputs_.shape\n",
    "print train_targets_.shape\n",
    "print test_inputs_.shape\n",
    "print test_targets_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels into sparse matrix for CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yr\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f044bcd3f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)\n",
    "print train_targets_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integer(labels):\n",
    "    labels_list = list(labels)\n",
    "    int_labels_list = map(lambda x: int(label_name_cls_map[x]), labels_list)\n",
    "    return int_labels_list\n",
    "\n",
    "train_targets_integer = map(convert_labels_to_integer, train_targets_)\n",
    "test_targets_integer = map(convert_labels_to_integer, test_targets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sparse_tensor(sparse_tensor):\n",
    "    \"\"\"Transform sparse to sequences ids.\"\"\"\n",
    "    decoded_indexes = list()\n",
    "    current_i = 0\n",
    "    current_seq = []\n",
    "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
    "        i = i_and_index[0]\n",
    "        if i != current_i:\n",
    "            decoded_indexes.append(current_seq)\n",
    "            current_i = i\n",
    "            current_seq = list()\n",
    "        current_seq.append(offset)\n",
    "    decoded_indexes.append(current_seq)\n",
    "\n",
    "    result = []\n",
    "    for index in decoded_indexes:\n",
    "        ids = [sparse_tensor[1][m] for m in index]\n",
    "        text = ''.join(list(map(id2word, ids)))\n",
    "        result.append(text)\n",
    "    return result\n",
    "    \n",
    "def id2word(idx):\n",
    "    return str(idx)\n",
    "\n",
    "def hit(text1, text2):\n",
    "    \"\"\"Calculate accuracy of predictive text and target text.\"\"\"\n",
    "    res = []\n",
    "    for idx, words1 in enumerate(text1):\n",
    "        res.append(words1 == text2[idx])\n",
    "    return np.mean(np.asarray(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 57]\n"
     ]
    }
   ],
   "source": [
    "print train_targets_integer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_len = [timesteps_size]*train_inputs_.shape[0] \n",
    "test_seq_len = [timesteps_size]*test_inputs_.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL LSTM + CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "num_examples = train_inputs_.shape[0]\n",
    "batch_size=9\n",
    "num_batches_per_epoch = num_examples/batch_size\n",
    "num_layers = 2\n",
    "\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cellS = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)   \n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "    \n",
    "    session =  tf.Session()\n",
    "\n",
    "    # Initializate the weights and biases\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in range(num_batches_per_epoch-1):\n",
    "            sparse_train_targets = sparse_tuple_from(train_targets_integers[batch*batch_size :(batch+1)*batch_size])\n",
    "            \n",
    "            feed = {inputs: train_inputs_[batch*batch_size :(batch+1)*batch_size],\n",
    "                    targets: sparse_train_targets,\n",
    "                    seq_len: train_seq_len[batch*batch_size :(batch+1)*batch_size]}\n",
    "\n",
    "            batch_cost, _, decoded = session.run([cost, optimizer, decoded], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "            \n",
    "            ori = decode_sparse_tensor(sparse_train_targets)\n",
    "            pre = decode_sparse_tensor(decoded[0])\n",
    "            acc = hit(pre, ori)\n",
    "            \n",
    "        train_cost /= num_examples\n",
    "        train_ler /= num_examples\n",
    "\n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, accuracy: {:.4f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler, acc, time.time() - start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replacing blank label to none\n",
    "str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
    "# Replacing space label to space\n",
    "str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
    "\n",
    "print('Original:\\n%s' % original)\n",
    "print('Decoded:\\n%s' % str_decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
