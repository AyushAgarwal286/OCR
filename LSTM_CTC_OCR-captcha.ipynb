{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import h5py\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining different variables\n",
    "data_dir = '../data/captcha/'\n",
    "img_width = 140\n",
    "img_height = 28\n",
    "timesteps_size = 140\n",
    "num_hidden_units = 256\n",
    "num_classes = 63\n",
    "\n",
    "# reading the maps\n",
    "label_cls_name_map = {}\n",
    "label_name_cls_map = {}\n",
    "with open('ocr_checkpoints/label_cls_name.json', 'r') as f:\n",
    "    label_cls_name_map = json.loads(f.read())\n",
    "    \n",
    "for k,v in label_cls_name_map.iteritems():\n",
    "    label_name_cls_map[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully !!!\n"
     ]
    }
   ],
   "source": [
    "# train dataset inputs and targets\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "train_image_names = os.listdir(os.path.join(data_dir,'train_images'))\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_train_annotations.csv'), \n",
    "                      header=None)\n",
    "for train_image_name in train_image_names:\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'train_images'), train_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0) # reading the image as 1 channel\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height), \n",
    "                          interpolation = cv2.INTER_AREA).T\n",
    "    train_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(train_df[train_df[0].map(\n",
    "        lambda x: x.split('/')[1])==train_image_name][1].tolist())\n",
    "    train_targets.append(target_)\n",
    "    \n",
    "# test dataset inputs and targets\n",
    "test_inputs = []\n",
    "test_targets = []\n",
    "test_image_names = os.listdir(os.path.join(data_dir,'test_images'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_test_annotations.csv'), \n",
    "                      header=None)\n",
    "for test_image_name in test_image_names:\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'test_images'), test_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0)\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height), \n",
    "                          interpolation = cv2.INTER_AREA).T\n",
    "    test_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(test_df[test_df[0].map(\n",
    "        lambda x: x.split('/')[1])==test_image_name][1].tolist())\n",
    "    test_targets.append(target_)\n",
    "\n",
    "train_inputs_ = np.array(train_inputs)\n",
    "train_targets_ = np.array(train_targets)\n",
    "test_inputs_ = np.array(test_inputs)\n",
    "test_targets_ = np.array(test_targets)\n",
    "print 'Data loaded successfully !!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 140, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0e00d54b50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e48443490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 140, 28)\n",
      "(20000,)\n",
      "(2000, 140, 28)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print train_inputs_.shape\n",
    "print train_targets_.shape\n",
    "print test_inputs_.shape\n",
    "print test_targets_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels into sparse matrix for CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3l8734\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABlCAYAAABdnhjZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEZVJREFUeJzt3XmYVNWZx/HvS9NNs7Y0CLZAQqMIoiJoIuhoYkQNuKESt+hI1Eg0bqgJmj3OM3ES4x6NhgRBHaImuBHHZRDNmKjgNg6CKBBAAVlUEAgg0M07f5zb95bQTTe91HL9fZ6nnzr3nFtVb92qOn3uOeeeMndHREQKX6tcByAiIs1DFbqISEqoQhcRSQlV6CIiKaEKXUQkJVShi4ikhCp0EZGUaFKFbmbDzexdM1tgZtc2V1AiIrLrrLEXFplZETAPOAZYCrwKnOXubzdfeCIi0lCtm3DfQ4AF7r4QwMweBEYCdVboJdbGS2nfhKcUEfn8Wc+aj9x99/r2a0qF3gNYkrG9FBiy/U5mNgYYA1BKO4bYsCY8pYjI58+zPuW9huzXlAq9Qdx9PDAeoJOV79i/M3QgABsr2rZ0KCIieavd8k0hMWNWox+jKYOiy4BeGds9ozwREcmBplTorwJ9zazSzEqAM4GpzROWiIjsqkZ3ubh7lZldCjwDFAH3uPucXX2ctT/dCMCMQfc1NhQRkYI39M1vAFB2XOMfo0l96O7+JPBkUx5DRESah64UFRFJCVXoIiIpoQpdRCQlVKGLiKSEKnQRkZRQhS4ikhKq0EVEUkIVuohISqhCFxFJCVXoIiIp0eLL50rL6f/3fwWg7V871rlPxWmL4/ST/bRKg0iaqYUuIpISqWyhr922KU6/trkDANUe/ncdVro+LuvQqjS7gTWz4/cKi1u+1LEyzls5vysAfS+fCcCaE/fKfmAikhNqoYuIpIQqdBGRlKi3y8XM7gFOAFa5+/5RXjnwENAbWAyc7u5rWi7MnVta9U8Ajp44DoDeTyTdKl5kABStDd0wtxQXxWXzxoXfMZ33tQlxXpEVzv+4myreCImaW+DL1acDYK3DW/u1PeZnPS4RyY2G1F6TgOHb5V0LTHf3vsD0aFtERHKo3ha6u79gZr23yx4JHBml7wX+ClzTjHHtku5FoaU96Oh3ABh+xuy47KDS9wH4oKoMgB//6vy4rP+1oez+Z/eI877VaVXLBtvCPl7UGYBuPSoA2K/dS7kMR0SyqLGzXLq7+/IovQLoXteOZjYGGANQSrtGPp2IiNSnydMW3d3NzHdSPh4YD9DJyuvcrymKLfSLP1j5XC2lYWriwJLNAFzxhaSkW2kJALu3XtcSYeVEh0XhWFRVhJZ6/5LlGaUlOYjo8215NL5z1MyL47xP1zRuuuxlh02P01eVL2xaYJJKjR0BXGlmFQDRbWH3U4iIpEBjK/SpwOgoPRp4vHnCERGRxmrItMUHCAOgXc1sKfAz4JfAn8zsAuA94PSWDHJXXffhgDj91LKQ/uTVbgDs9dBHcdnKW4oBOL7dp1mMrvmtqd4YpzsuqQZg0x7htL5f8bacxFSXzb41Th/x5jcBWPtGuLp1W3GyX9v+nwBwywF/ivOOLA33LaSppR9Uh26uqkUd4rzSDVbn/q2iw9P7j0vjPC8OX9PqR+u+X3OoeW8+9ao4r5jQhdeuVXq766o9+Y4si75LpRaOdbei9jmJqbEaMsvlrDqKhjVzLCIi0gSpXMvl3r8dEae7vRz+0+71QpiiuKXP7nHZvl0WZTewFvJBddJya780nG2s+nJoWeTbejWj5p8Up8tPDe9J0aldAFh1/Oa4bPPs3QC44Zdnx3nfPSG8pmnn/hqAL7ROWr356uA2oWU7/5y7drrfP7eF9+3wG64CoKpbWVw2YuLfABjbeXELRAgDXjoHgK7/GWahtV2RnLFu7RBOmz7ev02cV3HSewA8ss+jQOG33kctGBGnt5wYrQPVM0xlnvTMxLisEFrrhXPuKiIiO5XKFvrCU3+XbJwabj6q3gDAyVdeFRetPi/03b7ydNKve0ibjI7cArGiOmmpFi8JYwQbTsnPOf/n7flinL5n93AmtfKwMJt14VETd9h/1jlJa/HbP7sSgLOuvhqAKTfdFJdVFEBrfXuZYx/DfhFeU/c3wrIVl0x+OC47qf1GWtL1Bz4GwPfeCy314746Ny7rXfoxAJMXfSnO23BHTwCObXUZAOP+4/6sxdqcas6KPr6td5xXOngLACXzwnTfoY8l9cXCURn1Sp5SC11EJCVUoYuIpEQqu1xq0zUa0FjdP1ltsez10NWyflvmwGF1NsNqFh1bJd0SXha6Hnr8T5h6dvKQr8dlG6rC4FWfjqFb5nc9X85WiLFRHZKrcu+fHGLsf2GIp7JoTFz2zPG3ADCwJBmIOvC7swBYdkx4D0d+67y47JXBf26hiJtfzfTAwyZ8L86rfDac4p/5RBgAzWbXxcntw9WsJ591d537ZF6ZuvTAsP+pP/0+ALdenEyEO2DCbQBUFud/F9j1Hx4CQMcXFsR5Wx8KXZVvL+wFwIDrkumjjw0Pr6nmeOUjtdBFRFKiYFvo71cl/yXHLQlT4U7b/TUA9mj9SVy2oipMf/v5nBMA6H33u3HZwu/2A5ILVoLC+x93cEly1rH19tCy+3RiWMtl7fW94rKN3cOA7+LRLbKkzi57rO8zAEz+S5i2eP2kM+Kyy0d9B4AVh3aK89p9GC4A6bQprP++X5cVWYmzuQ16Kaz4WXlX0jJsNyW8tnM7fVTrfXJtzpbkZx2f27AfAJu6humy5ZPfisseXT8QKIy1Zh54dQgA/fdMfj/hF33CAG+ffcLg6Dl3J2eNPxl/LgAnjr0jzsu3i9zyKxoREWk0VegiIilRsF0uD6wdHKdXXdcHgN+v6QGA17LkRZcuYeBz3m3J+rkzvhKuOCyy/L8CbGcyT/um7fuXkLghR8E0wtkdw1znN09LBmnn3NkRgK3H7B/nrRwSriTd1CXMid7y4+TK0r1HhG6AR0bdGucNLMmfq2R/s+aLcbryh+GaiHnf7xPnze1zZ5QqIh89vm5QnJ427isA9FoS3jfvl7yO37wQ9ut37H1xXr6uldTllVD9rT6wc5y3f3HojmzXKtQJa/89ib3XuWFR2StOPzTOu6PHzBaPc1eohS4ikhIF20K/pkvy48dXTQw/Pbdga2ixrc6YhlgeTenbpzjkfXYQo7Bb5mmzoTpZL8Srw/TRPYYl08amD5gaEkeFmxmfJlNML7w7XLU4dsylcd4ld4SVGjOnSmbb2m1hMHHS7cfFeeV7hM/klFG3xXnF1oZ89sOuyWSCayaEK0k/jl7btI3JWe+P/34KALd/87Q479ILw3fvrRG/AXK/vtDGbWHAs+3qMBBdvC5ZXXK/Jy/5zL62NakvytuE9232TwfGeZWnhvSNX30IyO1nDdRCFxFJDXPP3hS2TlbuQ+yzq+6ufXJvAGYMmpK1OCQ/1bRmAYbcE9bQ6Pl80k9+yp3TALioLKz2l3m2tSpaq+eMC8fGeRu7hRPQ6deHi5Ry0TI87/2wXs2KY5K89yeFFu2cQydnPZ5suf6jfnH6xZP3BeCdy8MKhvNO+21clstpf2cuCqd6r728T4P2L14fYu194//FeeuOPwCAL14RzmD+WPl8o+MZ+uY3ACg7bsEOZc/6lNfd/Us7FGyn3qNpZr3M7Hkze9vM5pjZFVF+uZlNM7P50W3n+h5LRERaTkP+PVYBV7v7AGAocImZDQCuBaa7e19gerQtIiI50pBfLFoOLI/S681sLtADGEn4aTqAe4G/Ate0SJTyuVDWqm2cnnn+zQAcvPfFcd7U878GwITBYTB7zaBkMKvon2G6X785ySBq6fIw9XH2lnCF7NAcjMUdXhYG7x9/Ipn291RlzTKs+b/eSW2WRldp1/xwW20/NDK2POPq0SND10b/m5YA8Ptjk6uXL9ptWQtFWb8HK58LiZrbetQMph66LunW6/JW6BL8SY//inJyu2z1Ls1yMbPewGBgJtA9quwBVgDd67jPGGAMQGmOX6yISJo1uEI3sw7Aw8BYd19nlly94+5uZrWOrrr7eGA8hEHRpoUrnxc1rfUFR06K89YcEdapuWxJmAL44py+yR2iT9bbP6mIsy47bDoAQ0tzd7HOBWUrotunM3ILs2Ve492t4efxfv79bwPQfkmyMmRV+3A2VLIyWR+l/J0ZAGwadhAAvUvyc72a+tT81N64ix6K82658XQAbl4ZRr1/3+vFHe+YRQ0aYjazYkJlPtndH4myV5pZRVReAaxqmRBFRKQhGjLLxYAJwFx3vzmjaCowOkqPBh5v/vBERKSh6p2HbmaHA38D3iIZB/khoR/9T8AXgPeA09199c4eS/PQRdKj5jc5l1Rti/Oe2TAAgGWbk1nMB7QLg6Entn8fgM5FGkurTXPMQ2/ILJe/A7UsdwXAsDryRUQkywp2LRcRya2aK2/3LUny9i1ZHKUWb787uZ7S93mgtVxERFJCFbqISEqoQhcRSQlV6CIiKaEKXUQkJVShi4ikRM6nLZb9W5jKdETFd3IciYhI7pQt31T/TvVQC11EJCVUoYuIpETOu1yYMQvQNWQiIk2lFrqISErUu9pisz6Z2YfABqAwV7gPuqL4c6mQ4y/k2EHx59IX3X33+nbKaoUOYGavNWQZyHyl+HOrkOMv5NhB8RcCdbmIiKSEKnQRkZTIRYU+PgfP2ZwUf24VcvyFHDso/ryX9T50ERFpGepyERFJCVXoIiIpkdUK3cyGm9m7ZrbAzK7N5nPvKjPrZWbPm9nbZjbHzK6I8svNbJqZzY9uO9f3WLlkZkVm9r9m9kS0XWlmM6P34CEzK6nvMXLFzHYzsylm9o6ZzTWzQwvp+JvZldFnZ7aZPWBmpfl8/M3sHjNbZWazM/JqPd4W3B69jllmdlDuIo9jrS3+X0efn1lm9qiZ7ZZR9oMo/nfN7Ou5ibp5Za1CN7Mi4E5gBDAAOMvMBmTr+RuhCrja3QcAQ4FLonivBaa7e19gerSdz64A5mZs/wq4xd33BtYAF+Qkqoa5DXja3fsDBxJeR0EcfzPrAVwOfMnd9weKgDPJ7+M/CRi+XV5dx3sE0Df6GwPclaUYd2YSO8Y/Ddjf3QcC84AfAETf5TOB/aL7/DaqowpaNlvohwAL3H2hu28BHgRGZvH5d4m7L3f3N6L0ekJl0oMQ873RbvcCJ+cmwvqZWU/geOAP0bYBRwFTol3yNn4zKwO+AkwAcPct7v4JBXT8CWsltTWz1oTlipaTx8ff3V8AVm+XXdfxHgnc58EMYDczq8hOpLWrLX53/293r4o2ZwA9o/RI4EF33+zui4AFhDqqoGWzQu8BLMnYXhrl5T0z6w0MBmYC3d19eVS0Auieo7Aa4lZgHLAt2u4CfJLxAc/n96AS+BCYGHUZ/cHM2lMgx9/dlwE3Au8TKvK1wOsUzvGvUdfxLsTv8/nAU1G6EOOvlwZF62FmHYCHgbHuvi6zzMOcz7yc92lmJwCr3P31XMfSSK2Bg4C73H0wYQ2gz3Sv5Pnx70xoBVYCewLt2bE7oKDk8/Guj5n9iNCNOjnXsbSkbFboy4BeGds9o7y8ZWbFhMp8srs/EmWvrDm1jG5X5Sq+evwLcJKZLSZ0bx1F6JPeLeoCgPx+D5YCS919ZrQ9hVDBF8rxPxpY5O4fuvtW4BHCe1Iox79GXce7YL7PZvYt4ATgbE8uvCmY+HdFNiv0V4G+0Sh/CWFAYmoWn3+XRP3NE4C57n5zRtFUYHSUHg08nu3YGsLdf+DuPd29N+FYP+fuZwPPA9+Idsvn+FcAS8ysX5Q1DHibAjn+hK6WoWbWLvos1cRfEMc/Q13HeypwbjTbZSiwNqNrJm+Y2XBCt+NJ7r4xo2gqcKaZtTGzSsLg7iu5iLFZuXvW/oDjCCPN/wB+lM3nbkSshxNOL2cBb0Z/xxH6oacD84FngfJcx9qA13Ik8ESU7kP44C4A/gy0yXV8O4l7EPBa9B48BnQupOMPXAe8A8wG7gfa5PPxBx4g9PdvJZwhXVDX8QaMMGvtH8BbhNk8+Rj/AkJfec13+O6M/X8Uxf8uMCLX8TfHny79FxFJCQ2KioikhCp0EZGUUIUuIpISqtBFRFJCFbqISEqoQhcRSQlV6CIiKfH/cUB+unuG8GAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e00cffe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)\n",
    "print train_targets_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integer(labels):\n",
    "    labels_list = list(labels)\n",
    "    int_labels_list = map(lambda x: int(label_name_cls_map[x]), labels_list)\n",
    "    return int_labels_list\n",
    "\n",
    "train_targets_integer = map(convert_labels_to_integer, train_targets_)\n",
    "test_targets_integer = map(convert_labels_to_integer, test_targets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sparse_tensor(sparse_tensor):\n",
    "    \"\"\"Transform sparse to sequences ids.\"\"\"\n",
    "    decoded_indexes = list()\n",
    "    current_i = 0\n",
    "    current_seq = []\n",
    "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
    "        i = i_and_index[0]\n",
    "        if i != current_i:\n",
    "            decoded_indexes.append(current_seq)\n",
    "            current_i = i\n",
    "            current_seq = list()\n",
    "        current_seq.append(offset)\n",
    "    decoded_indexes.append(current_seq)\n",
    "\n",
    "    result = []\n",
    "    for index in decoded_indexes:\n",
    "        ids = [sparse_tensor[1][m] for m in index]\n",
    "        text = ''.join(list(map(id2word, ids)))\n",
    "        result.append(text)\n",
    "    return result\n",
    "    \n",
    "def id2word(idx):\n",
    "    return str(idx)\n",
    "\n",
    "def hit(text1, text2):\n",
    "    \"\"\"Calculate accuracy of predictive text and target text.\"\"\"\n",
    "    res = []\n",
    "    for idx, words1 in enumerate(text1):\n",
    "        res.append(words1 == text2[idx])\n",
    "    return np.mean(np.asarray(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 38, 1, 15, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print train_targets_integer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_len = [timesteps_size]*train_inputs_.shape[0] \n",
    "test_seq_len = [timesteps_size]*test_inputs_.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL LSTM + CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "num_examples = train_inputs_.shape[0]\n",
    "batch_size=50\n",
    "num_batches_per_epoch = num_examples/batch_size\n",
    "num_layers = 2\n",
    "\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)   \n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "    \n",
    "    session =  tf.Session()\n",
    "\n",
    "    # Initializate the weights and biases\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = acc = 0\n",
    "        start = time.time()\n",
    "        for batch in range(num_batches_per_epoch-1):\n",
    "            sys.stdout.write('\\r%d/%d'% (batch, num_batches_per_epoch))\n",
    "            sparse_train_targets = sparse_tuple_from(train_targets_integer[batch*batch_size :(batch+1)*batch_size])\n",
    "            feed = {inputs: train_inputs_[batch*batch_size :(batch+1)*batch_size],\n",
    "                    targets: sparse_train_targets,\n",
    "                    seq_len: train_seq_len[batch*batch_size :(batch+1)*batch_size]}\n",
    "\n",
    "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            decoded_ = session.run(decoded, feed_dict=feed)\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "            ori = decode_sparse_tensor(sparse_train_targets)\n",
    "            pre = decode_sparse_tensor(decoded_[0])\n",
    "            acc += hit(pre, ori)*batch_size\n",
    "            \n",
    "        train_cost /= num_examples\n",
    "        train_ler /= num_examples\n",
    "        acc /= num_examples\n",
    "        \n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, accuracy: {:.4f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler, acc, time.time() - start))\n",
    "        saver = tf.train.Saver()\n",
    "        save_dir = 'ocr_checkpoints_lstm/'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, 'best_validation_' + str(curr_epoch+1))\n",
    "        saver.save(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    \n",
    "    session =  tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path='ocr_checkpoints_lstm/best_validation_49')\n",
    "    \n",
    "    feed = {inputs: train_inputs_[0:2], seq_len: train_seq_len[0:2]}\n",
    "    decoded_ = session.run(decoded, feed_dict=feed)\n",
    "    pre = decode_sparse_tensor(decoded_[0])\n",
    "    print pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_inputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
