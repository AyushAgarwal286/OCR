{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import h5py\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining different variables\n",
    "data_dir = '../data/captcha/'\n",
    "img_width = 140\n",
    "img_height = 28\n",
    "timesteps_size = 140\n",
    "num_hidden_units = 256\n",
    "num_classes = 63\n",
    "\n",
    "# reading the maps\n",
    "label_cls_name_map = {}\n",
    "label_name_cls_map = {}\n",
    "with open('ocr_checkpoints/label_cls_name.json', 'r') as f:\n",
    "    label_cls_name_map = json.loads(f.read())\n",
    "    \n",
    "for k,v in label_cls_name_map.iteritems():\n",
    "    label_name_cls_map[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully !!!\n"
     ]
    }
   ],
   "source": [
    "# train dataset inputs and targets\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "train_image_names = os.listdir(os.path.join(data_dir,'train_images'))\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_train_annotations.csv'), \n",
    "                      header=None)\n",
    "for i, train_image_name in enumerate(train_image_names[:10]):\n",
    "    sys.stdout.write('\\r%d' % i)\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'train_images'), train_image_name)\n",
    "    image_np = cv2.imread(full_image_path) # reading the image as 1 channel\n",
    "    #image_np = cv2.resize(image_np, (img_width, img_height), \n",
    "    #                      interpolation = cv2.INTER_AREA)\n",
    "    train_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(train_df[train_df[0].map(\n",
    "        lambda x: x.split('/')[1])==train_image_name][1].tolist())\n",
    "    train_targets.append(target_)\n",
    "    \n",
    "# test dataset inputs and targets\n",
    "test_inputs = []\n",
    "test_targets = []\n",
    "test_image_names = os.listdir(os.path.join(data_dir,'test_images'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_test_annotations.csv'), \n",
    "                      header=None)\n",
    "for i, test_image_name in enumerate(test_image_names[:10]):\n",
    "    sys.stdout.write('\\r%d' % i)\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'test_images'), test_image_name)\n",
    "    image_np = cv2.imread(full_image_path)\n",
    "    #image_np = cv2.resize(image_np, (img_width, img_height), \n",
    "    #                      interpolation = cv2.INTER_AREA)\n",
    "    test_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(test_df[test_df[0].map(\n",
    "        lambda x: x.split('/')[1])==test_image_name][1].tolist())\n",
    "    test_targets.append(target_)\n",
    "\n",
    "train_inputs_ = np.array(train_inputs)\n",
    "train_targets_ = np.array(train_targets)\n",
    "test_inputs_ = np.array(test_inputs)\n",
    "test_targets_ = np.array(test_targets)\n",
    "print '\\rData loaded successfully !!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 80, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f65e9379bd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65e97fcb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 80, 300, 3)\n",
      "(10,)\n",
      "(10, 80, 300, 3)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print train_inputs_.shape\n",
    "print train_targets_.shape\n",
    "print test_inputs_.shape\n",
    "print test_targets_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels into sparse matrix for CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3l8734\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65e87742d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0])\n",
    "print train_targets_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integer(labels):\n",
    "    labels_list = list(labels)\n",
    "    int_labels_list = map(lambda x: int(label_name_cls_map[x]), labels_list)\n",
    "    return int_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sparse_tensor(sparse_tensor):\n",
    "    \"\"\"Transform sparse to sequences ids.\"\"\"\n",
    "    decoded_indexes = list()\n",
    "    current_i = 0\n",
    "    current_seq = []\n",
    "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
    "        i = i_and_index[0]\n",
    "        if i != current_i:\n",
    "            decoded_indexes.append(current_seq)\n",
    "            current_i = i\n",
    "            current_seq = list()\n",
    "        current_seq.append(offset)\n",
    "    decoded_indexes.append(current_seq)\n",
    "\n",
    "    result = []\n",
    "    for index in decoded_indexes:\n",
    "        ids = [sparse_tensor[1][m] for m in index]\n",
    "        text = ''.join(list(map(id2word, ids)))\n",
    "        result.append(text)\n",
    "    return result\n",
    "    \n",
    "def id2word(idx):\n",
    "    return label_cls_name_map[str(idx)]\n",
    "\n",
    "def hit(text1, text2):\n",
    "    \"\"\"Calculate accuracy of predictive text and target text.\"\"\"\n",
    "    res = []\n",
    "    for idx, words1 in enumerate(text1):\n",
    "        res.append(words1 == text2[idx])\n",
    "    return np.mean(np.asarray(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 38, 1, 15, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "train_targets_integer = map(convert_labels_to_integer, train_targets_)\n",
    "test_targets_integer = map(convert_labels_to_integer, test_targets_)\n",
    "\n",
    "print train_targets_integer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 80, 300, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL LSTM + CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ctc_loss() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f4bb44f347dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ctc_loss() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "num_examples = train_inputs_.shape[0]\n",
    "img_height = train_inputs_.shape[1]\n",
    "img_width = train_inputs_.shape[2]\n",
    "num_channels = train_inputs_.shape[3]\n",
    "batch_size=2\n",
    "num_batches_per_epoch = num_examples/batch_size\n",
    "num_layers = 2\n",
    "\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(tf.float32, [None, img_width, img_height, num_channels])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(inputs=inputs, name='layer_conv1', padding='same', \n",
    "                             filters=16, kernel_size=5, activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, name='layer_conv2', padding='same', \n",
    "                             filters=32, kernel_size=5, activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=2, strides=2)\n",
    "    lstm_input = tf.contrib.layers.flatten(pool2)\n",
    "    new_lstm_input = tf.reshape(tensor=lstm_input, shape=[-1,int(lstm_input.shape[1]),1])\n",
    "    max_timesteps = int(new_lstm_input.shape[1])\n",
    "    num_hidden_units = max_timesteps\n",
    "    \n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, new_lstm_input, seq_len, dtype=tf.float32)\n",
    "    \n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_size, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(cost)   \n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "    \n",
    "    session =  tf.Session()\n",
    "\n",
    "    # Initializate the weights and biases\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path='ocr_checkpoints_lstm/best_validation_300')\n",
    "    \n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = acc = 0\n",
    "        start = time.time()\n",
    "        for batch in range(num_batches_per_epoch-1):\n",
    "            sys.stdout.write('\\r%d/%d'% (batch, num_batches_per_epoch))\n",
    "            sparse_train_targets = sparse_tuple_from(train_targets_integer[batch*batch_size :(batch+1)*batch_size])\n",
    "            feed = {inputs: train_inputs_[batch*batch_size :(batch+1)*batch_size],\n",
    "                    targets: sparse_train_targets,\n",
    "                    seq_len: [max_timesteps]*batch_size}\n",
    "\n",
    "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            decoded_ = session.run(decoded, feed_dict=feed)\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "            ori = decode_sparse_tensor(sparse_train_targets)\n",
    "            #print 'ori:', ori\n",
    "            pre = decode_sparse_tensor(decoded_[0])\n",
    "            #print 'pre:', pre\n",
    "            acc += hit(pre, ori)*batch_size\n",
    "            \n",
    "        train_cost /= num_examples\n",
    "        train_ler /= num_examples\n",
    "        acc /= num_examples\n",
    "        \n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, accuracy: {:.4f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler, acc, time.time() - start))\n",
    "        save_dir = 'ocr_checkpoints_lstm/'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, 'best_validation_' + str(curr_epoch+1))\n",
    "        saver.save(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "num_layers = 2\n",
    "\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    \n",
    "    session =  tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path='ocr_checkpoints_lstm/best_validation_300')\n",
    "    \n",
    "    feed = {inputs: train_inputs_[:4], seq_len: train_seq_len[:4]}\n",
    "    decoded_ = session.run(decoded, feed_dict=feed)\n",
    "    pre = decode_sparse_tensor(decoded_[0])\n",
    "    print pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_inputs_[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
